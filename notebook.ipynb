{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample size\n",
    "\n",
    "We can't test on all of them. Let's find an acceptable sample size and isolate some\n",
    "We'll pull from geometry - their solution had the most trouble w/ geometry, and lots of variance on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Gemotry - t\n",
    "folder_path = \"./MATH/test/geometry/\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Store each in list\n",
    "json_objects = []\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data[\"file_path\"]=file_path # Add file path so we can keep track of them easily\n",
    "        json_objects.append(json_data)\n",
    "\n",
    "len(json_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only do the hard ones - these are the ones they test in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'problem': \"A solid $5\\\\times 5\\\\times 5$ cube is composed of unit cubes. Each face of the large, solid cube is partially painted with gray paint, as shown. [asy]\\n\\nfill((0,0)--(0,1)--(1,1)--(1,0)--cycle,gray);\\n\\nfill((0,4)--(0,5)--(1,5)--(1,4)--cycle,gray);\\n\\nfill((4,1)--(5,1)--(5,0)--(4,0)--cycle,gray);\\n\\nfill((1,2)--(2,2)--(2,1)--(1,1)--cycle,gray);\\n\\nfill((2,2)--(3,2)--(3,1)--(2,1)--cycle,gray);\\n\\nfill((3,2)--(4,2)--(4,1)--(3,1)--cycle,gray);\\n\\nfill((1,3)--(2,3)--(2,2)--(1,2)--cycle,gray);\\n\\nfill((3,3)--(4,3)--(4,2)--(3,2)--cycle,gray);\\n\\nfill((1,4)--(2,4)--(2,3)--(1,3)--cycle,gray);\\n\\nfill((2,4)--(3,4)--(3,3)--(2,3)--cycle,gray);\\n\\nfill((3,4)--(4,4)--(4,3)--(3,3)--cycle,gray);\\n\\nfill((4,5)--(5,5)--(5,4)--(4,4)--cycle,gray);\\n\\ndraw((0,0)--(0,1)--(1,1)--(1,0)--(0,0),rgb(0,0,0));\\n\\ndraw((0,1)--(0,2)--(1,2)--(1,1),rgb(0,0,0));\\n\\ndraw((0,2)--(0,3)--(1,3)--(1,2),rgb(0,0,0));\\n\\ndraw((0,3)--(0,4)--(1,4)--(1,3),rgb(0,0,0));\\n\\ndraw((0,4)--(0,5)--(1,5)--(1,4),rgb(0,0,0));\\n\\ndraw((1,0)--(1,1)--(2,1)--(2,0)--(1,0),rgb(0,0,0));\\n\\ndraw((2,1)--(3,1)--(3,0)--(2,0),rgb(0,0,0));\\n\\ndraw((3,1)--(4,1)--(4,0)--(3,0),rgb(0,0,0));\\n\\ndraw((4,1)--(5,1)--(5,0)--(4,0),rgb(0,0,0));\\n\\ndraw((1,2)--(2,2)--(2,1)--(1,1),rgb(0,0,0));\\n\\ndraw((2,2)--(3,2)--(3,1)--(2,1)--(2,2),rgb(0,0,0));\\n\\ndraw((3,2)--(4,2)--(4,1),rgb(0,0,0));\\n\\ndraw((4,2)--(5,2)--(5,1)--(4,1),rgb(0,0,0));\\n\\ndraw((1,3)--(2,3)--(2,2)--(1,2)--(1,3),rgb(0,0,0));\\n\\ndraw((2,3)--(3,3)--(3,2),rgb(0,0,0));\\n\\ndraw((3,3)--(4,3)--(4,2),rgb(0,0,0));\\n\\ndraw((4,3)--(5,3)--(5,2),rgb(0,0,0));\\n\\ndraw((1,4)--(2,4)--(2,3),rgb(0,0,0));\\n\\ndraw((2,4)--(3,4)--(3,3),rgb(0,0,0));\\n\\ndraw((3,4)--(4,4)--(4,3),rgb(0,0,0));\\n\\ndraw((4,4)--(5,4)--(5,3),rgb(0,0,0));\\n\\ndraw((1,5)--(2,5)--(2,4),rgb(0,0,0));\\n\\ndraw((2,5)--(3,5)--(3,4),rgb(0,0,0));\\n\\ndraw((3,5)--(4,5)--(4,4),rgb(0,0,0));\\n\\ndraw((4,5)--(5,5)--(5,4),rgb(0,0,0));\\n\\n[/asy] \\t \\tWhat fraction of the entire solid cube's unit cubes have no paint on them? Express your answer as a common fraction.\", 'level': 'Level 5', 'type': 'Geometry', 'solution': 'We know that each of the unit cubes in the $3\\\\times3\\\\times3$ cube in the center of the $5\\\\times5\\\\times5$ cube has no paint on it. On the surface of the cube, three of the unit cubes on each edge of the big cube have no paint on them, and the center unit cube of each face of the big cube has no paint on it. Since a cube has $12$ edges and $6$ faces, this makes a total of $3\\\\cdot3\\\\cdot3 + 12\\\\cdot3 + 6\\\\cdot1 = 69$ unit cubes with no paint on them. There are $125$ unit cubes altogether. The fraction with no paint is $\\\\boxed{\\\\frac{69}{125}}.$', 'file_path': './MATH/test/geometry/396.json'}]\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "filtered_json_objects = [obj for obj in json_objects if obj.get('level') == 'Level 5']\n",
    "print(filtered_json_objects[:1])\n",
    "print(len(filtered_json_objects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 132. HOw many of these at random do we need to pick to get a reasonable estimate?\n",
    "\n",
    "We can use the finite sample size forumla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size: 56\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Copied from equation here: https://online.stat.psu.edu/stat415/lesson/6/6.3\n",
    "def calculate_sample_size(N, p_hat, Z, E):\n",
    "    m = (Z**2 * p_hat * (1 - p_hat)) / (E**2)\n",
    "    n = m / (1 + ((m - 1) / N))\n",
    "\n",
    "    return math.ceil(n)\n",
    "\n",
    "N = len(filtered_json_objects) # 132 for geometry\n",
    "p_hat = 0.5      # Estimated proportion - unknown so use .50\n",
    "Z = 1.96         # Z-score for 95% confidence level\n",
    "E = 0.10         # Desired margin of error \n",
    "\n",
    "# Calculate the required sample size\n",
    "required_sample_size = calculate_sample_size(N, p_hat, Z, E)\n",
    "print(f\"Required sample size: {required_sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at a 10% margin of error we would still need to run our test 56 times.. that'll cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$15.40\n"
     ]
    }
   ],
   "source": [
    "# 20-35 cents per run\n",
    "print(f\"${required_sample_size * ((.20+.35)/2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such is the price we pay for progress. \n",
    "\n",
    "Let's isolate 56 random from the filtered_json_objects, and save this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "./MATH/test/geometry/393.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(7) # lucky number 7\n",
    "\n",
    "length = len(filtered_json_objects)\n",
    "\n",
    "sampled = random.sample(filtered_json_objects, required_sample_size)\n",
    "\n",
    "print(len(sampled))\n",
    "print(sampled[0][\"file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 objects at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "Let's take 5 for testing. And do a run of 3. This way we can compare the variances. \n",
    "We're going to log them and then verify them manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok.. what do I want to do now?? Let's see.. I want to make a space to save the logs of each run, and then run on each one of these problems. The name of the log should include the file path. \n",
    "\n",
    "I'll have to change my logging code to log to somewhere specific. hmm...\n",
    "\n",
    "Then I want to try running it on how it is now.\n",
    "Then I want to run it w/ only using the code assistant for the executor. Maybe this will help reduce costs and not be that much more :)\n",
    "I can also try running w/ gpt 4o-mini on some of the simple tasks. e.g. summarizing. \n",
    "\n",
    "Then I can see which ones it's failing on. And see how they're both doing. And then we can go problem by problem and see what tweaks can fix things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['problem', 'level', 'type', 'solution', 'file_path'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "samples = random.sample(filtered_json_objects, 5)\n",
    "samples[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# Unfortunately this clears the output, but used to avoid running this (40 minute) process each time\n",
    "\n",
    "\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main1.main import main\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# Do 3 runs\n",
    "for i in range(3):\n",
    "    # Do 5 problems\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        max_times_mining_new = 1  # The upper limit of the mining times\n",
    "        question = problem[\"problem\"]\n",
    "        main(question, max_times_mining_new, coding_assistant, coding_thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "New ideas:\n",
    "- Seems like we fail on the steps being wrong - try checking these & improving the prompt to not calculate yet\n",
    "1. Add a verify_steps function, and improve the steps prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import re\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main2.main import main # THIS IS THE ONLY DIFFERENCE\n",
    "\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# Make nested arr\n",
    "verify_arr = [f\"Problem {i} -> \" for i,_ in enumerate(samples)]\n",
    "# 3 runs of 5 problems\n",
    "for i in range(3):\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        max_times_mining_new = 1  # The upper limit of the mining times\n",
    "        question = problem[\"problem\"]\n",
    "        our_answer = main(question, max_times_mining_new, coding_assistant, coding_thread)\n",
    "\n",
    "\n",
    "        # TODO: fix the capture group - currently it says everything is incorrect\n",
    "        # Validate\n",
    "    \n",
    "        # This long regex *should* recursively balance the ending parentheses\n",
    "        actual_answer = re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "        if actual_answer:\n",
    "            CustomLogger.default_log(\"Actual Answer\", actual_answer)\n",
    "            if True:\n",
    "                CustomLogger.default_log(\"Correct\")\n",
    "                verify_arr[j]+= \"correct \"\n",
    "        \n",
    "        else:\n",
    "            CustomLogger.default_log(\"Incorrect\")\n",
    "            verify_arr[j]+= \"incorrect \"\n",
    "            \n",
    "\n",
    "# Log verify arr -> validation\n",
    "CustomLogger.update_path(\"validation\")\n",
    "CustomLogger.default_log(\"Results\", *verify_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- The re-assessment of steps we added is good. But for instance in run 0 problem 3, the reassessment fails twice in a row so we just kep going. We need to explain why it fails and then add that context to the step creator.\n",
    "1. To do this we can ask it to explain why its failing - let's do a schema w/ true false and reason. Since schema doesn't generate intermediate, lets do two calls, one normal and one schema. \n",
    "\n",
    "- Also in problem 3, I think we end up with conditions that I think were contradictory. To improve this we can do two things. \n",
    "1. Generate less conditions each time, and then increase the amount of mining loops we do\n",
    "2. After finishing all the mining, ask if any conditions are contradictory. If they are, return those indices\n",
    "\n",
    "\n",
    "\n",
    "Changes for main 3-\n",
    "1. added reasoning for failed steps, to be passed to step creator in the form of optional_comment\n",
    "2. Updated verify steps to use schema & tweaked prompt\n",
    "3. Limit conditions to generate each time both by asking GPT and through slicing\n",
    "\n",
    "4. Experimented w/ a double check of conditions, but got into a bit of a snarl. If two conditions are contradictory, how do we know which is true? We have to re-wind a lot, and it gets confusing. This might not work without a drastic overhaul of the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import re\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main3.main import main # THIS IS THE ONLY DIFFERENCE\n",
    "\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# This long regex *should* recursively balance the ending parentheses\n",
    "def get_boxed(problem):\n",
    "    return re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "\n",
    "MAX_MINING = 3 # Bumped this up!\n",
    "\n",
    "\n",
    "generated_answers = [f\"Problem {i} -> \" for i,_ in enumerate(samples)]\n",
    "actual_answers= [f\"Problem {i} -> {get_boxed(problem)}\" for i, problem in enumerate(samples)]\n",
    "# 3 runs of 5 problems\n",
    "for i in range(3):\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        question = problem[\"problem\"]\n",
    "        our_answer = main(question, MAX_MINING, coding_assistant, coding_thread)\n",
    "        generated_answers[j]+= f\"{our_answer}, \"\n",
    "\n",
    "        CustomLogger.default_log(\"Correct Answer:\", get_boxed(problem))\n",
    "            \n",
    "# TODO: It would be cool to auto compare them - however the actual answers are given in formula form, e.g. 3/5. \n",
    "CustomLogger.update_path(\"validation\")\n",
    "CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "CustomLogger.default_log(\"Actual\", *actual_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from main 3**\n",
    "\n",
    "- There's at least one instance of the executor failing to execute correctly. Lets add in a quick sanity check after the executor finishes. This sanity check, if it fails, triggers a re-run of the execution plus a reason (DO and DO NOT) for why it failed\n",
    "\n",
    "- The logs are getting way too long. I need to trim them down. I'll probably switch to logging in main. I want to keep the full logs somewhere though, I just also want to have non-full logs.\n",
    "\n",
    "- I also want to switch to async since right now it takes forever..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "import re\n",
    "from chains.main4.main import main4\n",
    "from utils.create_assistant import create_agents_and_thread, WorkSpace\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.async_logger import AsyncLogger\n",
    "from contextlib import asynccontextmanager\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class WorkSpacePool():\n",
    "    def __init__(self, workspaces: list[WorkSpace]):\n",
    "        self.workspaces = deque(workspaces)\n",
    "        self.semaphore = anyio.Semaphore(len(workspaces))\n",
    "\n",
    "    @asynccontextmanager\n",
    "    async def get_space(self):\n",
    "        async with self.semaphore:\n",
    "            workspace = self.workspaces.popleft()\n",
    "            try:\n",
    "                yield workspace\n",
    "            finally:\n",
    "                self.workspaces.append(workspace)\n",
    "\n",
    "def get_boxed(problem):\n",
    "    return re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "\n",
    "MAX_MINING = 3 \n",
    "generated_answers = [f\"Problem {i} -> \" for i, _ in enumerate(samples)]\n",
    "actual_answers= [f\"Problem {i} -> {get_boxed(problem)}\" for i, problem in enumerate(samples)]\n",
    "failed_samples = []\n",
    "results = []  \n",
    "\n",
    "# Async wrapper - modifies global results to get the results out of the async\n",
    "async def wrapper(pool: WorkSpacePool, question, log_path, j):\n",
    "    async with pool.get_space() as workspace:\n",
    "        try:\n",
    "            # Attempt to get result from main4\n",
    "            result = await main4(question, MAX_MINING, workspace.assistant, workspace.thread, log_path)\n",
    "            results.append(result)\n",
    "            generated_answers[j] += f\"{result}, \" #TODO: the ordering in the generated answers may be incorrect b.c. async \n",
    "        except Exception as e:\n",
    "            # Handle main4 failure, log it, and store the problem for later re-run\n",
    "            CustomLogger.print(f\"Error on problem {j} in run {log_path}: {e}\")\n",
    "            failed_samples.append((log_path, question, j))  #TODO: do something w/ the failed samples\n",
    "            AsyncLogger.add_message(log_path, \"Error\", str(e))\n",
    "            AsyncLogger.flush_one(log_path)\n",
    "\n",
    "async def main():\n",
    "    #TODO: switch to individual timedelta for each run...\n",
    "    # Record start time so we can get timedelta for each run.\n",
    "    CustomLogger.start_watch() \n",
    "\n",
    "    # Limit the number of concurrent tasks to avoid rate limits & code session charges :)\n",
    "    workspaces = [create_agents_and_thread() for _ in range(5)]\n",
    "    pool = WorkSpacePool(workspaces)\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "       for i in range(3):\n",
    "            for j, problem in enumerate(samples):\n",
    "                log_path = f\"run-{i}/problem-{j}\"\n",
    "                AsyncLogger.add_message(log_path, \"Problem File Path\", problem[\"file_path\"])\n",
    "                question = problem[\"problem\"]\n",
    "                tg.start_soon(wrapper, pool, question, log_path, j) \n",
    "    \n",
    "    CustomLogger.print(\"All answers collected.\")\n",
    "\n",
    "    CustomLogger.update_path(\"validation\")\n",
    "    CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "    CustomLogger.default_log(\"Actual\", *actual_answers)\n",
    "\n",
    "\n",
    "try:\n",
    "    await main()\n",
    "except Exception as main_exception:\n",
    "    CustomLogger.print(f\"Exception in main: {main_exception}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! I was able to make it asynchronous. I have a couple ideas of what to do next.\n",
    "\n",
    "1. My first is that I think the way I've been validating is less than ideal. Instead of running on 15 I can instead run the same problem ~5 or so times. And I can aim for getting the same problem correct every time. If I can improve its percentage of being correct, I can track progress better than 5x3. I can even run the same problem 10 times for the same cost as before. \n",
    "\n",
    "2. Next - I started thinking about the idea behind this process in the first place. The idea is to break it into smaller logical chunks and verify those. However, we get incorrect sections of logic. Sometimes we have an incorrect formula, or its applied incorrectly, or sometimes conditions contradict each other.\n",
    "\n",
    "It would be good to be able to trace back our steps in logic - see what conditions each other condition is built off of.\n",
    "- We could try generating one condition at a time, and see what that looks like. And then build a history. This history is important so we can roll back and check. And periodically we can walk through and check our work. \n",
    "\n",
    "3. When we write the steps for the end, it would be good to standardize the language. More than that, it would be good to standardize what each step uses. Something like Use [technique or condition] along with [conditions] to calculate [step answer] which is needed because [reason]\n",
    "\n",
    "4. It would be nice to have mini objectives. I'm looking at the solution for one of the problems written out. It's something like:\n",
    "    - Draw diagram. Triangle is equilateral. Because of that, it's area is <area formula>\n",
    "    - Draw on the diagram to show this given representation. *notice* that this makes 3 new triangles. We can sum their areas to find the total area. This gives us an equation.\n",
    "    \n",
    "    Each step has an objective, or a couple. It's setting up the problem for us. The condition gathering tries to mimic this process - each condition is supposed to be basically a mini step. However, we generate multiple at a time, and we don't differentiate which ones came first. So we treat them all with the same level. We need a chronological order. This way we can see what leads to other conditions.\n",
    "\n",
    "5. Are the original extracted conditions always good? Can we get some way of getting pretty much always the same extracted conditions from the problem? If we run it e.g. 5 times and pull them together can we achieve this? We can also consider going back and getting more conditions from the problem - sometimes we will hit steps that will cause us to need to know more problem info.\n",
    "    - Hmm.. so its impossible to know ahead of time exactly which conditions will be needed. We can pull some, but I think as we go through the problem we will want to revise the problem statement and get new conditions. We should do this in general I think - revising early parts to see what else we can get from it. Again, we need the history. I think to have effective history we need a specific structure - like reason, calculation, assumption.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can get the extracted conditions to be the same..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the diagram, four squares of side length 2 are placed in the corners of a square of side length 6.  Each of the points $W$, $X$, $Y$, and $Z$ is a vertex of one of the small squares.  Square $ABCD$ can be constructed with sides passing through $W$, $X$, $Y$, and $Z$. What is the maximum possible distance from $A$ to $P$? [asy]\n",
      "path square = scale(2) * unitsquare;\n",
      "\n",
      "draw(square); draw(shift(4) * square); draw(shift(4, 4) * square); draw(shift(0, 4) * square);\n",
      "draw((2, 0)--(4, 0)); draw((0, 2)--(0, 4)); draw((6, 2)--(6, 4)); draw((2, 6)--(4, 6));\n",
      "\n",
      "pair a = shift(3, 4) * dir(135);\n",
      "pair b = shift(4, 3) * dir(45);\n",
      "pair c = shift(3, 2) * dir(-45);\n",
      "pair d = shift(2, 3) * dir(-135);\n",
      "draw(a--b--c--d--cycle);\n",
      "\n",
      "label(\"$2$\", (1, 6), N); label(\"$2$\", (3, 6), N); label(\"$2$\", (5, 6), N);\n",
      "\n",
      "label(\"$2$\", (6, 5), E); label(\"$2$\", (6, 3), E); label(\"$2$\", (6, 1), E);\n",
      "\n",
      "label(\"$W$\", (2, 4), NW); label(\"$X$\", (4, 4), NE); label(\"$Y$\", (4, 2), SE); label(\"$Z$\", (2, 2), SW);\n",
      "label(\"$A$\", a, N); label(\"$B$\", b, E); label(\"$C$\", c, S); label(\"$D$\", d, W); label(\"$P$\", (6, 0), SE);\n",
      "[/asy]\n"
     ]
    }
   ],
   "source": [
    "# Pick a question for now - sometimes we get problem 3 right, but not often.\n",
    "\n",
    "\n",
    "question = samples[3]['problem']\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from macm.executor import execute_steps\n",
    "from macm.judge import (\n",
    "    verify_new_condition,\n",
    "    check_answer,\n",
    "    verify_steps,\n",
    "    double_check_conditions,\n",
    ")\n",
    "from macm.thinker import (\n",
    "    extract_from_original,\n",
    "    new_conditions_from_existing,\n",
    "    create_steps,\n",
    ")\n",
    "from utils.helpers import (\n",
    "    conditions_objectives_to_string,\n",
    "    list_to_numbered_string,\n",
    ")\n",
    "from utils.async_logger import AsyncLogger\n",
    "from utils.custom_logger import CustomLogger\n",
    "\n",
    "log_path = 'problem3-1'\n",
    "\n",
    "# Can run async outside main in nb\n",
    "for i in range(5):\n",
    "    conditions, objectives = await extract_from_original(question)\n",
    "    c_str, o_str = conditions_objectives_to_string(conditions, objectives)\n",
    "    AsyncLogger.add_message(\n",
    "        log_path, \"Extracted from problem, (conditions, objectives)\", c_str, o_str, \"\\n\"\n",
    "    )\n",
    "\n",
    "AsyncLogger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: evaluate the boxed answer so we can auto compare..\n",
    "\n",
    "# from sympy.parsing.latex import parse_latex\n",
    "\n",
    "# # Example LaTeX string\n",
    "# latex_string = r\"\\frac{5}{3}\"\n",
    "\n",
    "# # Parse the LaTeX string\n",
    "# expr = parse_latex(latex_string)\n",
    "\n",
    "# # Evaluate the expression\n",
    "# print(\"Exact result:\", expr)\n",
    "# print(\"Floating-point result:\", expr.evalf())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
