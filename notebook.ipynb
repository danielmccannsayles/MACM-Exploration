{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample size\n",
    "\n",
    "We can't test on all of them. Let's find an acceptable sample size and isolate some\n",
    "We'll pull from geometry - their solution had the most trouble w/ geometry, and lots of variance on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Gemotry - t\n",
    "folder_path = \"./MATH/test/geometry/\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Store each in list\n",
    "json_objects = []\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data[\"file_path\"]=file_path # Add file path so we can keep track of them easily\n",
    "        json_objects.append(json_data)\n",
    "\n",
    "len(json_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only do the hard ones - these are the ones they test in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'problem': \"A solid $5\\\\times 5\\\\times 5$ cube is composed of unit cubes. Each face of the large, solid cube is partially painted with gray paint, as shown. [asy]\\n\\nfill((0,0)--(0,1)--(1,1)--(1,0)--cycle,gray);\\n\\nfill((0,4)--(0,5)--(1,5)--(1,4)--cycle,gray);\\n\\nfill((4,1)--(5,1)--(5,0)--(4,0)--cycle,gray);\\n\\nfill((1,2)--(2,2)--(2,1)--(1,1)--cycle,gray);\\n\\nfill((2,2)--(3,2)--(3,1)--(2,1)--cycle,gray);\\n\\nfill((3,2)--(4,2)--(4,1)--(3,1)--cycle,gray);\\n\\nfill((1,3)--(2,3)--(2,2)--(1,2)--cycle,gray);\\n\\nfill((3,3)--(4,3)--(4,2)--(3,2)--cycle,gray);\\n\\nfill((1,4)--(2,4)--(2,3)--(1,3)--cycle,gray);\\n\\nfill((2,4)--(3,4)--(3,3)--(2,3)--cycle,gray);\\n\\nfill((3,4)--(4,4)--(4,3)--(3,3)--cycle,gray);\\n\\nfill((4,5)--(5,5)--(5,4)--(4,4)--cycle,gray);\\n\\ndraw((0,0)--(0,1)--(1,1)--(1,0)--(0,0),rgb(0,0,0));\\n\\ndraw((0,1)--(0,2)--(1,2)--(1,1),rgb(0,0,0));\\n\\ndraw((0,2)--(0,3)--(1,3)--(1,2),rgb(0,0,0));\\n\\ndraw((0,3)--(0,4)--(1,4)--(1,3),rgb(0,0,0));\\n\\ndraw((0,4)--(0,5)--(1,5)--(1,4),rgb(0,0,0));\\n\\ndraw((1,0)--(1,1)--(2,1)--(2,0)--(1,0),rgb(0,0,0));\\n\\ndraw((2,1)--(3,1)--(3,0)--(2,0),rgb(0,0,0));\\n\\ndraw((3,1)--(4,1)--(4,0)--(3,0),rgb(0,0,0));\\n\\ndraw((4,1)--(5,1)--(5,0)--(4,0),rgb(0,0,0));\\n\\ndraw((1,2)--(2,2)--(2,1)--(1,1),rgb(0,0,0));\\n\\ndraw((2,2)--(3,2)--(3,1)--(2,1)--(2,2),rgb(0,0,0));\\n\\ndraw((3,2)--(4,2)--(4,1),rgb(0,0,0));\\n\\ndraw((4,2)--(5,2)--(5,1)--(4,1),rgb(0,0,0));\\n\\ndraw((1,3)--(2,3)--(2,2)--(1,2)--(1,3),rgb(0,0,0));\\n\\ndraw((2,3)--(3,3)--(3,2),rgb(0,0,0));\\n\\ndraw((3,3)--(4,3)--(4,2),rgb(0,0,0));\\n\\ndraw((4,3)--(5,3)--(5,2),rgb(0,0,0));\\n\\ndraw((1,4)--(2,4)--(2,3),rgb(0,0,0));\\n\\ndraw((2,4)--(3,4)--(3,3),rgb(0,0,0));\\n\\ndraw((3,4)--(4,4)--(4,3),rgb(0,0,0));\\n\\ndraw((4,4)--(5,4)--(5,3),rgb(0,0,0));\\n\\ndraw((1,5)--(2,5)--(2,4),rgb(0,0,0));\\n\\ndraw((2,5)--(3,5)--(3,4),rgb(0,0,0));\\n\\ndraw((3,5)--(4,5)--(4,4),rgb(0,0,0));\\n\\ndraw((4,5)--(5,5)--(5,4),rgb(0,0,0));\\n\\n[/asy] \\t \\tWhat fraction of the entire solid cube's unit cubes have no paint on them? Express your answer as a common fraction.\", 'level': 'Level 5', 'type': 'Geometry', 'solution': 'We know that each of the unit cubes in the $3\\\\times3\\\\times3$ cube in the center of the $5\\\\times5\\\\times5$ cube has no paint on it. On the surface of the cube, three of the unit cubes on each edge of the big cube have no paint on them, and the center unit cube of each face of the big cube has no paint on it. Since a cube has $12$ edges and $6$ faces, this makes a total of $3\\\\cdot3\\\\cdot3 + 12\\\\cdot3 + 6\\\\cdot1 = 69$ unit cubes with no paint on them. There are $125$ unit cubes altogether. The fraction with no paint is $\\\\boxed{\\\\frac{69}{125}}.$', 'file_path': './MATH/test/geometry/396.json'}]\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "filtered_json_objects = [obj for obj in json_objects if obj.get('level') == 'Level 5']\n",
    "print(filtered_json_objects[:1])\n",
    "print(len(filtered_json_objects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 132. HOw many of these at random do we need to pick to get a reasonable estimate?\n",
    "\n",
    "We can use the finite sample size forumla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size: 56\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Copied from equation here: https://online.stat.psu.edu/stat415/lesson/6/6.3\n",
    "def calculate_sample_size(N, p_hat, Z, E):\n",
    "    m = (Z**2 * p_hat * (1 - p_hat)) / (E**2)\n",
    "    n = m / (1 + ((m - 1) / N))\n",
    "\n",
    "    return math.ceil(n)\n",
    "\n",
    "N = len(filtered_json_objects) # 132 for geometry\n",
    "p_hat = 0.5      # Estimated proportion - unknown so use .50\n",
    "Z = 1.96         # Z-score for 95% confidence level\n",
    "E = 0.10         # Desired margin of error \n",
    "\n",
    "# Calculate the required sample size\n",
    "required_sample_size = calculate_sample_size(N, p_hat, Z, E)\n",
    "print(f\"Required sample size: {required_sample_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at a 10% margin of error we would still need to run our test 56 times.. that'll cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$15.40\n"
     ]
    }
   ],
   "source": [
    "# 20-35 cents per run\n",
    "print(f\"${required_sample_size * ((.20+.35)/2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such is the price we pay for progress. \n",
    "\n",
    "Let's isolate 56 random from the filtered_json_objects, and save this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "./MATH/test/geometry/393.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(7) # lucky number 7\n",
    "\n",
    "length = len(filtered_json_objects)\n",
    "\n",
    "sampled = random.sample(filtered_json_objects, required_sample_size)\n",
    "\n",
    "print(len(sampled))\n",
    "print(sampled[0][\"file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 objects at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "Let's take 5 for testing. And do a run of 3. This way we can compare the variances. \n",
    "We're going to log them and then verify them manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok.. what do I want to do now?? Let's see.. I want to make a space to save the logs of each run, and then run on each one of these problems. The name of the log should include the file path. \n",
    "\n",
    "I'll have to change my logging code to log to somewhere specific. hmm...\n",
    "\n",
    "Then I want to try running it on how it is now.\n",
    "Then I want to run it w/ only using the code assistant for the executor. Maybe this will help reduce costs and not be that much more :)\n",
    "I can also try running w/ gpt 4o-mini on some of the simple tasks. e.g. summarizing. \n",
    "\n",
    "Then I can see which ones it's failing on. And see how they're both doing. And then we can go problem by problem and see what tweaks can fix things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['problem', 'level', 'type', 'solution', 'file_path'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "samples = random.sample(filtered_json_objects, 5)\n",
    "samples[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# Unfortunately this clears the output, but used to avoid running this (40 minute) process each time\n",
    "\n",
    "\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main1.main import main\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# Do 3 runs\n",
    "for i in range(3):\n",
    "    # Do 5 problems\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        max_times_mining_new = 1  # The upper limit of the mining times\n",
    "        question = problem[\"problem\"]\n",
    "        main(question, max_times_mining_new, coding_assistant, coding_thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "New ideas:\n",
    "- Seems like we fail on the steps being wrong - try checking these & improving the prompt to not calculate yet\n",
    "1. Add a verify_steps function, and improve the steps prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import re\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main2.main import main # THIS IS THE ONLY DIFFERENCE\n",
    "\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# Make nested arr\n",
    "verify_arr = [f\"Problem {i} -> \" for i,_ in enumerate(samples)]\n",
    "# 3 runs of 5 problems\n",
    "for i in range(3):\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        max_times_mining_new = 1  # The upper limit of the mining times\n",
    "        question = problem[\"problem\"]\n",
    "        our_answer = main(question, max_times_mining_new, coding_assistant, coding_thread)\n",
    "\n",
    "\n",
    "        # TODO: fix the capture group - currently it says everything is incorrect\n",
    "        # Validate\n",
    "    \n",
    "        # This long regex *should* recursively balance the ending parentheses\n",
    "        actual_answer = re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "        if actual_answer:\n",
    "            CustomLogger.default_log(\"Actual Answer\", actual_answer)\n",
    "            if True:\n",
    "                CustomLogger.default_log(\"Correct\")\n",
    "                verify_arr[j]+= \"correct \"\n",
    "        \n",
    "        else:\n",
    "            CustomLogger.default_log(\"Incorrect\")\n",
    "            verify_arr[j]+= \"incorrect \"\n",
    "            \n",
    "\n",
    "# Log verify arr -> validation\n",
    "CustomLogger.update_path(\"validation\")\n",
    "CustomLogger.default_log(\"Results\", *verify_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- The re-assessment of steps we added is good. But for instance in run 0 problem 3, the reassessment fails twice in a row so we just kep going. We need to explain why it fails and then add that context to the step creator.\n",
    "1. To do this we can ask it to explain why its failing - let's do a schema w/ true false and reason. Since schema doesn't generate intermediate, lets do two calls, one normal and one schema. \n",
    "\n",
    "- Also in problem 3, I think we end up with conditions that I think were contradictory. To improve this we can do two things. \n",
    "1. Generate less conditions each time, and then increase the amount of mining loops we do\n",
    "2. After finishing all the mining, ask if any conditions are contradictory. If they are, return those indices\n",
    "\n",
    "\n",
    "\n",
    "Changes for main 3-\n",
    "1. added reasoning for failed steps, to be passed to step creator in the form of optional_comment\n",
    "2. Updated verify steps to use schema & tweaked prompt\n",
    "3. Limit conditions to generate each time both by asking GPT and through slicing\n",
    "\n",
    "4. Experimented w/ a double check of conditions, but got into a bit of a snarl. If two conditions are contradictory, how do we know which is true? We have to re-wind a lot, and it gets confusing. This might not work without a drastic overhaul of the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import re\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from chains.main3.main import main # THIS IS THE ONLY DIFFERENCE\n",
    "\n",
    "\n",
    "# Use the same agents and threads to (ideally) limit code sessions\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "# This long regex *should* recursively balance the ending parentheses\n",
    "def get_boxed(problem):\n",
    "    return re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "\n",
    "MAX_MINING = 3 # Bumped this up!\n",
    "\n",
    "\n",
    "generated_answers = [f\"Problem {i} -> \" for i,_ in enumerate(samples)]\n",
    "actual_answers= [f\"Problem {i} -> {get_boxed(problem)}\" for i, problem in enumerate(samples)]\n",
    "# 3 runs of 5 problems\n",
    "for i in range(3):\n",
    "    for j, problem in enumerate(samples):\n",
    "        CustomLogger.update_path(f\"run-{i}/problem-{j}\")\n",
    "        CustomLogger.default_log(\"Problem File Path\", problem[\"file_path\"])\n",
    "\n",
    "        question = problem[\"problem\"]\n",
    "        our_answer = main(question, MAX_MINING, coding_assistant, coding_thread)\n",
    "        generated_answers[j]+= f\"{our_answer}, \"\n",
    "\n",
    "        CustomLogger.default_log(\"Correct Answer:\", get_boxed(problem))\n",
    "            \n",
    "# TODO: It would be cool to auto compare them - however the actual answers are given in formula form, e.g. 3/5. \n",
    "CustomLogger.update_path(\"validation\")\n",
    "CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "CustomLogger.default_log(\"Actual\", *actual_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from main 3**\n",
    "\n",
    "- There's at least one instance of the executor failing to execute correctly. Lets add in a quick sanity check after the executor finishes. This sanity check, if it fails, triggers a re-run of the execution plus a reason (DO and DO NOT) for why it failed\n",
    "\n",
    "- The logs are getting way too long. I need to trim them down. I'll probably switch to logging in main. I want to keep the full logs somewhere though, I just also want to have non-full logs.\n",
    "\n",
    "- I also want to switch to async since right now it takes forever..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=BrokenPipeError(32, 'Broken pipe')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmccanns/miniforge3/envs/macm/lib/python3.10/asyncio/unix_events.py\", line 676, in write\n",
      "    n = os.write(self._fileno, data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "import anyio\n",
    "import re\n",
    "from chains.main4.main import main4\n",
    "from utils.create_assistant import create_agents_and_thread\n",
    "from utils.custom_logger import CustomLogger\n",
    "from utils.async_logger import AsyncLogger\n",
    "\n",
    "# This *should* work since the coding assistants are synchronous\n",
    "coding_assistant, coding_thread = create_agents_and_thread()\n",
    "\n",
    "def get_boxed(problem):\n",
    "    return re.search(r'\\\\boxed{((?:[^{}]+|{(?:[^{}]+|{[^{}]*})*})*)}', problem[\"solution\"]).group(1)\n",
    "\n",
    "MAX_MINING = 3 \n",
    "generated_answers = [f\"Problem {i} -> \" for i, _ in enumerate(samples)]\n",
    "actual_answers= [f\"Problem {i} -> {get_boxed(problem)}\" for i, problem in enumerate(samples)]\n",
    "failed_samples = []\n",
    "results = []  \n",
    "\n",
    "# Async wrapper - modifies results to get the results out of the async\n",
    "async def wrapper(semaphore, question, log_path, j):\n",
    "    async with semaphore: # Limit concurrent tasks\n",
    "        try:\n",
    "            # Attempt to get result from main4\n",
    "            result = await main4(question, MAX_MINING, coding_assistant, coding_thread, log_path)\n",
    "            results.append(result)\n",
    "            generated_answers[j] += f\"{result}, \" #TODO: the ordering in the generated answers may be incorrect b.c. async \n",
    "        except Exception as e:\n",
    "            # Handle main4 failure, log it, and store the problem for later re-run\n",
    "            CustomLogger.print(f\"Error on problem {j} in run {log_path}: {e}\")\n",
    "            failed_samples.append((log_path, question, j))  #TODO: do something w/ the failed samples\n",
    "            AsyncLogger.add_message(log_path, \"Error\", str(e))\n",
    "            AsyncLogger.flush_one(log_path)\n",
    "\n",
    "async def main():\n",
    "    #TODO: switch to individual timedelta for each run...\n",
    "    # Record start time so we can get timedelta for each run.\n",
    "    CustomLogger.start_watch() \n",
    "\n",
    "    # Limit the number of concurrent tasks w/ semaphore to avoid rate limits\n",
    "    max_concurrent_tasks = 10  \n",
    "    semaphore = anyio.Semaphore(max_concurrent_tasks)\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "       for i in range(3):\n",
    "            for j, problem in enumerate(samples):\n",
    "                log_path = f\"run-{i}/problem-{j}\"\n",
    "                AsyncLogger.add_message(log_path, \"Problem File Path\", problem[\"file_path\"])\n",
    "                question = problem[\"problem\"]\n",
    "                tg.start_soon(wrapper, semaphore, question, log_path,j) \n",
    "    \n",
    "    CustomLogger.print(\"All answers collected.\")\n",
    "\n",
    "    CustomLogger.update_path(\"validation\")\n",
    "    CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "    CustomLogger.default_log(\"Actual\", *actual_answers)\n",
    "\n",
    "\n",
    "try:\n",
    "    await main()\n",
    "except Exception as main_exception:\n",
    "    CustomLogger.print(f\"Exception in main: {main_exception}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hihello\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\" \n",
    "      \"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: evaluate the boxed answer so we can auto compare..\n",
    "\n",
    "# from sympy.parsing.latex import parse_latex\n",
    "\n",
    "# # Example LaTeX string\n",
    "# latex_string = r\"\\frac{5}{3}\"\n",
    "\n",
    "# # Parse the LaTeX string\n",
    "# expr = parse_latex(latex_string)\n",
    "\n",
    "# # Evaluate the expression\n",
    "# print(\"Exact result:\", expr)\n",
    "# print(\"Floating-point result:\", expr.evalf())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
