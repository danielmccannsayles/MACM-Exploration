{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "132\n",
      "Point $P$ is inside equilateral triangle $ABC$ such that the altitudes from $P$ to $\\overline{AB}$, $\\overline{BC}$, and $\\overline{CA}$ have lengths 5, 6, and 7 respectively.  What is the area of triangle $ABC$?\n",
      "./MATH/test/geometry/990.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Gemotry - t\n",
    "folder_path = \"./MATH/test/geometry/\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Store each in list\n",
    "json_objects = []\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data[\"file_path\"]=file_path # Add file path so we can keep track of them easily\n",
    "        json_objects.append(json_data)\n",
    "\n",
    "print(len(json_objects))\n",
    "filtered_json_objects = [obj for obj in json_objects if obj.get('level') == 'Level 5']\n",
    "print(len(filtered_json_objects))\n",
    "\n",
    "# Get 5 random ones\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "samples = random.sample(filtered_json_objects, 5)\n",
    "\n",
    "question = samples[0]['problem']\n",
    "print(question)\n",
    "print(samples[0][\"file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 1 - Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I should try asking it to solve to the end to start, and have it return a list of steps. Then I can go back and see how many are correct?\n",
    "We can limit this by only having it create max 5 steps.\n",
    "\n",
    "Then we can go through step by step. For each step->\n",
    "\n",
    "** For now, avoid this recursive breaking up **\n",
    "\n",
    "1. Should we break this further into steps? is it too detailed?\n",
    "    1. If so, repeat this process and recurse\n",
    "2. If not, then we analyze this step, and extract the following things\n",
    "    - Proven mathematical relationships - written as formulas\n",
    "    - Problem relationships - written without math, just words. Some sort of logical condition in the project\n",
    "    - Intermediate math relationships - numerical relationships between parts of the problem. Only valid in this context. Uses a math formula and some specific knowledge. Break up into a proven math rel and a problem rel\n",
    "\n",
    "3. We need to verify the above. \n",
    "    - Proven math relations can be verified by asking\n",
    "    - Problem relationships verified by asking as well\n",
    "    - Intermediate math relationships should have any math relations verified, then any problem relationships verified, then finally they should be executed and verified with code\n",
    "\n",
    "4. Now the step is good to go. We can re-write it to show the verified above things. We can also store the proven relationships and problem relationships seperately. \n",
    "\n",
    "5. Now if at any point in the larger step we had a problem, we are going to have to recalculate the rest of the steps. If it's the same we can continue through the steps, but if its different it may change the next ones.\n",
    "    - I think this is fine. We need some way of error correcting, and propogating that correction\n",
    "\n",
    "6. Now this is all very good for fixing problems with steps, but what if the whole approach is wrong in some way? Eventually we will build up a list of correct steps, but not all of them will be relevant. And there may be a better order? for some. (Though the order should be fixed by regenerating steps each time.. hmm)\n",
    "Not sure what to do *yet* here\n",
    "\n",
    "Original step by step idea->\n",
    "1. Ask what it would like to do - what is the first step, given what we have.\n",
    "2. Then analyze the step it returns - what is the rationale behind it? What outside theorems does it use? What new assumptions does it make?\n",
    "3. Check each assumption individually, and iterate until we get a good first step.\n",
    "4. Then do this step, update the knowns, and then repeat the process until we get the objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Try and solve the original problem, using max steps. Have it format them w/ schema.\n",
    "# Go step by step..\n",
    "from utils.async_gpt import agenerate_from_gpt_with_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Steps(BaseModel):\n",
    "    steps: list[str]\n",
    "    summary: str\n",
    "\n",
    "create_steps_prompt = \"\"\"\n",
    "Given the question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\n",
    "Return a series of the next steps to solve the problem. \n",
    "Return a maximum of {steps_left} steps, but it's okay to return less, even just 1 step if that's all it takes to get the solution.\n",
    "Be detailed, and break up complex steps into multiple steps. Try and balance the complexity of each step.\n",
    "Additionally, give a brief summary on the overall strategy, or any key points\n",
    "\"\"\"\n",
    "\n",
    "async def create_steps(question: str, prev_steps: list[str], steps_left:int):\n",
    "    \"\"\"\n",
    "    Create steps to solve a question\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"And the previous steps:\\n {numbered_list}\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": create_steps_prompt.format(question=question, prev_steps_str=prev_steps_str, steps_left=steps_left),\n",
    "        },\n",
    "    ]\n",
    "    steps: Steps = await agenerate_from_gpt_with_schema(\n",
    "        messages, Steps\n",
    "    )\n",
    "\n",
    "    return steps.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I change the question manually, it seems to get it wrong. I should check this out by running 20 or so times each. \n",
    "# THis is kind of expected, after reading that paper about how the models are likely learning common problems...\n",
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Let's go through step by step and see if each step is good. \n",
    "# Generate an explanation for why or why not it's correct. Then return true/false. If any step is wrong, we re-calculate the next steps\n",
    "from utils.async_gpt import agenerate_from_gpt_with_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class VerifyStep(BaseModel):\n",
    "    reasoning: str\n",
    "    correct: bool\n",
    "\n",
    "\n",
    "verify_step_prompt = \"\"\"\n",
    "Given the following info, verify if the CURRENT STEP is correct. \n",
    "Assume the question and any previous steps are correct.\n",
    "ONLY VERIFY the current step.\n",
    "Return the reasoning for why or why not it is correct, and a bool for if it is correct or not\n",
    "\n",
    "Existing Info ====\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\n",
    "New ====\n",
    "Current step:\n",
    "{current_step}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "async def verify_step(question: str, prev_steps: list[str], current_step: str):\n",
    "    \"\"\"\n",
    "    Verify a step given the previous ones. \n",
    "    * Note: we do not care if this step is helpful towards the objective. We're just checking the assumptions it makes.\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        # Turn prev steps to str\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"Previous Steps:\\n {numbered_list}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": verify_step_prompt.format(question=question, prev_steps_str=prev_steps_str, current_step=current_step),\n",
    "        },\n",
    "    ]\n",
    "    verify: VerifyStep = await agenerate_from_gpt_with_schema(\n",
    "        messages, VerifyStep\n",
    "    )\n",
    "\n",
    "    return verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 If a step is wrong, we'll want to correct it.\n",
    "# For now lets try giving the function all the info. We will give it the existing info, the previous incorrect step, and the reasoning behind it\n",
    "from utils.async_gpt import agenerate_from_gpt\n",
    "\n",
    "\n",
    "previous_info_prompt = \"\"\"\n",
    "Existing True Information ====\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\"\"\"\n",
    "\n",
    "wrong_step_prompt = \"\"\"\n",
    "New Information ===\n",
    "Current step (incorrect):\n",
    "{current_step}\n",
    "\n",
    "Provided reason why current step is incorrect:\n",
    "{reasoning}\n",
    "\"\"\"\n",
    "\n",
    "ask=\"\"\"\n",
    "The existing true information has been verified. It shows the preceding steps.\n",
    "The new information shows the generated current step. This current step contains an error in it. The reason for the error is given.\n",
    "\n",
    "Your job is to rewrite this current step so that it is CORRECT. Use the reason given to fix the current step.\n",
    "Refer back to the existing true information for verified assumptions. Do not add ANY new ideas, or more steps. \n",
    "ONLY change the current step.\n",
    "Fix the current step for the reason provided, and return ONLY the new current step.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def fix_step(question: str, prev_steps: list[str], current_step: str, reasoning: str):\n",
    "    \"\"\"\n",
    "    Fix a step that was said to be wrong. Returns the new step.\n",
    "    Currently we are giving all the information. In the future consider limiting scope\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        # Turn prev steps to str\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"Previous Steps:\\n {numbered_list}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"assistant\",\n",
    "            \"content\": previous_info_prompt.format(question=question, prev_steps_str=prev_steps_str)},\n",
    "\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": wrong_step_prompt.format(current_step=current_step, reasoning=reasoning),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": ask,\n",
    "        },\n",
    "    ]\n",
    "    new_step = await agenerate_from_gpt(\n",
    "        messages\n",
    "    )\n",
    "\n",
    "    return new_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get final answer from steps\n",
    "\n",
    "async def get_answer_from_steps(steps: list[str]):\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(steps)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" Given the following solution steps to a problem return ONLY the final answer: \n",
    "            {steps_str}\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    return await agenerate_from_gpt(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.async_logger import AsyncLogger\n",
    "\n",
    "async def step_by_step_loop(question: str, log_path:str):\n",
    "    AsyncLogger.add_message(log_path, f\"Starting run {log_path}\",)\n",
    "    # Max steps is used to try and keep gpt from generating a million steps or 1 step each time.\n",
    "    MAX_STEPS = 5\n",
    "    verified_steps = []\n",
    "    # Create the first steps\n",
    "    unverified_steps = await create_steps(question, [], MAX_STEPS)\n",
    "\n",
    "    while len(unverified_steps) > 0 and len(verified_steps) < MAX_STEPS:\n",
    "        current_step = unverified_steps.pop(0)\n",
    "        AsyncLogger.add_message(log_path, f\"Verify the current step: \\n{current_step}\")\n",
    "        verify = await verify_step(question, verified_steps, current_step)\n",
    "\n",
    "        if not verify.correct:\n",
    "            AsyncLogger.add_message(log_path, f\"Fix the current step for reasoning: {verify.reasoning}\")\n",
    "            fixed_step = await fix_step(question, verified_steps, current_step, verify.reasoning)\n",
    "            AsyncLogger.add_message(log_path, f\"#Fixed step: {fixed_step}\")\n",
    "\n",
    "            # After fixing a step we need to verify it since it often gets off the rails. \n",
    "            # If it's correct we can add it. Else we throw the current step out and regenerate\n",
    "            verify_fixed = await verify_step(question, verified_steps, fixed_step)\n",
    "            AsyncLogger.add_message(log_path, f\"Fixed step was verified as: {verify_fixed.correct}\")\n",
    "    \n",
    "            if verify_fixed.correct:\n",
    "                verified_steps.append(fixed_step)\n",
    "                if len(verified_steps) == MAX_STEPS: # If we have reached the end, break here. #TODO: would be nice to avoid checking len(verified_steps) in two places..\n",
    "                    break\n",
    "            \n",
    "            unverified_steps = await create_steps(question, verified_steps, MAX_STEPS - len(verified_steps))\n",
    "            AsyncLogger.add_message(log_path, f\"Regenerated steps: \\n{unverified_steps}\")\n",
    "\n",
    "        else:\n",
    "            AsyncLogger.add_message(log_path, \"Step was correct. Add to verified\")\n",
    "            verified_steps.append(current_step)\n",
    "\n",
    "\n",
    "    AsyncLogger.add_message(log_path, \"We have our verified steps:==========\\n\")\n",
    "\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n{step}\" for i, step in enumerate(verified_steps)])\n",
    "    AsyncLogger.add_message(log_path, steps_str)\n",
    "\n",
    "    final_answer = await get_answer_from_steps(verified_steps)\n",
    "    AsyncLogger.add_message(log_path, f\"And our Final Answer\\n{final_answer}\")\n",
    "\n",
    "    await AsyncLogger.flush_one(log_path)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async wrapper to run multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "from utils.custom_logger import CustomLogger\n",
    "from typing import Callable, Awaitable\n",
    "\n",
    "\n",
    "async def run_many_times_async(function: Callable[[str, str], Awaitable[str]], question: str, max_concurrent_tasks=10, runs=10):\n",
    "    \"\"\" Run the given function many times. Logs results\"\"\"\n",
    "    generated_answers = []\n",
    "    CustomLogger.start_watch() \n",
    "\n",
    "    # Used in case we need to limit for e.g. rate limits\n",
    "    semaphore = anyio.Semaphore(max_concurrent_tasks)\n",
    "\n",
    "    # Async wrapper - edits generated_answers\n",
    "    async def wrapper(question: str, i: int):\n",
    "        log_path = f\"run-{i}\"\n",
    "        async with semaphore: # Limit concurrent tasks\n",
    "            try:\n",
    "                result = await function(question, log_path)\n",
    "                generated_answers.append(f\"{i}: {result}\")\n",
    "                CustomLogger.print(f\"Finished task {i}\") \n",
    "            except Exception as e:\n",
    "                CustomLogger.print(f\"Error on problem {log_path}: {e}\")\n",
    "                AsyncLogger.add_message(log_path, \"Error\", str(e))\n",
    "                await AsyncLogger.flush_one(log_path)\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "       for i in range(runs):\n",
    "            tg.start_soon(wrapper, question, i) \n",
    "    \n",
    "\n",
    "    CustomLogger.print(\"All answers collected.\")\n",
    "\n",
    "    # Add to validation\n",
    "    CustomLogger.update_path(\"validation\")\n",
    "    CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "    CustomLogger.default_log(\"Actual\", \"147 * sqrt(3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run \n",
    "(skipped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# Test out custom questions\n",
    "# Answer to q1 is 147*sqrt(3)\n",
    "question1 = \"Point $P$ is inside equilateral triangle $ABC$ such that the altitudes from $P$ to $\\overline{AB}$, $\\overline{BC}$, and $\\overline{CA}$ have lengths 8, 6, and 7 respectively.  What is the area of triangle $ABC$?\"\n",
    "\n",
    "\n",
    "await run_many_times_async(step_by_step_loop, question1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 2: Take Step by Step and improve on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of failed in nb2_output/idea 1/README.md\n",
    "\n",
    "Fixes needed:\n",
    "- To fix the math we can add a second validator that looks for mathematical inconsistencies using the code editor.\n",
    "- To fix the premature stopping we can add a check at the end that makes sure we have the exact answer. If not, we will add a step.\n",
    "- To fix the big steps being created we can \n",
    "    - change create_steps to always create the given amount and ask it to equalize them\n",
    "    - have a check for if a step is too large. If it is we want to break it up.\n",
    "\n",
    "For now we're just going to do one change to avoid cluttering it. We can see how this does, and see what problems it still faces. \n",
    "\n",
    "Implementation:\n",
    "- Changing create_steps to always create exact amount, try and balance steps, and not create summary.\n",
    "- Change the loop to keep running. After MIN, check if we have the answer\n",
    "    - This allows us to have create steps always create the exact amount of steps, since even if it creates too many steps we will check along the way.\n",
    "    - If it's not the last step, we will always make at least one more step. Even if we already hit max steps.\n",
    "    - This check should happen after the verify step.\n",
    "\n",
    "Future:\n",
    "- In idea 2 output we'll look for math errors and steps being created that are too large\n",
    "- The math stuff we can eventually sort each step verification into math, assumptions, etc.\n",
    "- The too large steps I'm thinking will occur more. Ideally I want to break them up recursively, but dunno how to practically do this yet. Let's see if its even necessary :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite create_steps to always return exact amount asked for\n",
    "from utils.async_gpt import agenerate_from_gpt_with_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class StepObj(BaseModel):\n",
    "    steps: list[str]\n",
    "\n",
    "create_steps_prompt = \"\"\"\n",
    "Given the question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\n",
    "Return a series of the next steps to solve the problem, ending in the solution. \n",
    "Return exactly {desired_steps} step(s), balancing the complexity evenly among them.\n",
    "Be detailed, and break up complex steps. \n",
    "\"\"\"\n",
    "\n",
    "async def create_steps_2(question: str, prev_steps: list[str], desired_steps:int):\n",
    "    \"\"\"\n",
    "    Create steps to solve a question. Returns exact amount of desired steps. \n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"And the previous steps:\\n {numbered_list}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": create_steps_prompt.format(question=question, prev_steps_str=prev_steps_str, desired_steps=desired_steps),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Keep generating until we get the exact amount\n",
    "    while True:\n",
    "        step_obj: StepObj = await agenerate_from_gpt_with_schema(\n",
    "            messages, StepObj\n",
    "        )\n",
    "        steps = step_obj.steps\n",
    "        if len(steps) == desired_steps:\n",
    "            break\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have answer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class HaveAnswer(BaseModel):\n",
    "    objective: str\n",
    "    answer: str\n",
    "    answered: bool\n",
    "\n",
    "answer_check_prompt = \"\"\" \n",
    "What is the given Problem asking? return only the objective.\n",
    "What is the specific answer given by the Text? Does it have one? If there's no specific, exact answer, return answer:\"\"\n",
    "\n",
    "If the Text contains the EXACT ANSWER to the objective, then return answered: True.\n",
    "Else, in all other cases, return answered: False\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Text:\n",
    "{step}\n",
    "\"\"\"\n",
    "\n",
    "async def do_we_have_answer(problem:str, step: str):\n",
    "    # Function to get final answer rom steps\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": answer_check_prompt.format(problem=problem, step=step),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    have_answer: HaveAnswer = await agenerate_from_gpt_with_schema(messages, HaveAnswer)\n",
    "    return have_answer.answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite get answer from steps to just get answer from last step\n",
    "# Before, since we weren't checking if each step had the exact answer, we had to check the whole thing. \n",
    "# Now, since have_answer triggered we can just check the last step\n",
    "# Function to get final answer from steps\n",
    "\n",
    "final_answer_prompt = \"\"\" \n",
    "Return ONLY the final answer to the Problem from the given Text.\n",
    "The answer should be no more than a line long. Return ONLY the EXACT answer given in the Text.\n",
    "\n",
    "Given the Problem: \n",
    "{problem}\n",
    "            \n",
    "And the Text\n",
    "{step}\n",
    "\"\"\"\n",
    "\n",
    "async def get_final_answer(problem:str, step: str,):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_answer_prompt.format(problem=problem, step=step),\n",
    "        },\n",
    "    ]\n",
    "    return await agenerate_from_gpt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - given the propensity to think we have the answer too early, let's do one final sanity check\n",
    "\n",
    "class TrueOrFalse(BaseModel):\n",
    "    value: bool\n",
    "\n",
    "sanity_prompt =\"\"\" \n",
    "Given the Problem: \n",
    "{problem}\n",
    "\n",
    "The Text:\n",
    "{step}\n",
    "            \n",
    "And the extracted Answer\n",
    "{answer}\n",
    "\n",
    "Return True if and only if the Text contains the Answer word for word, AND the Answer EXACTLY ANSWERS the Question\n",
    "\"\"\"\n",
    "\n",
    "async def sanity_check(problem:str, step: str, answer: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_answer_prompt.format(problem=problem, step=step, answer=answer),\n",
    "        },\n",
    "    ]\n",
    "    true_or_false: TrueOrFalse = await agenerate_from_gpt_with_schema(messages, TrueOrFalse)\n",
    "    return true_or_false.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New do we have answer\n",
    "# Reasoning - I want to look at all the steps. Now its not triggering enough LOL.\n",
    "\n",
    "class HaveAnswer2(BaseModel):\n",
    "    value: bool\n",
    "\n",
    "answer_2_prompt =\"\"\" \n",
    "Your job is to check if we have the exact answer, or if we need to take more steps to solve the problem.\n",
    "Return True if and only if the steps contain the exact answer that the problem asks for\n",
    "If the steps are finished, and give an exact answer to the Problem, return True.\n",
    "Else, if the steps do not give an exact answer (have unsolved equations, etc.) then return False\n",
    "\n",
    "Given the Problem: \n",
    "{problem}\n",
    "\n",
    "And the steps to solve it:\n",
    "{steps_str}\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "async def do_we_have_answer2(problem:str, steps: list[str]):\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(steps)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": answer_2_prompt.format(problem=problem, steps_str=steps_str),\n",
    "        },\n",
    "    ]\n",
    "    true_or_false: HaveAnswer2 = await agenerate_from_gpt_with_schema(messages, HaveAnswer2)\n",
    "    return true_or_false.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting this to include the question\n",
    "\n",
    "final_answer_prompt2 = \"\"\" \n",
    "Return ONLY the final answer to the Problem from the given Steps.\n",
    "The answer should be no more than a line long. Return ONLY the EXACT answer given in the Text.\n",
    "\n",
    "Given the Problem: \n",
    "{problem}\n",
    "            \n",
    "And the Steps\n",
    "{steps_str}\n",
    "\"\"\"\n",
    "\n",
    "async def get_answer_from_steps2(problem, steps: list[str]):\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(steps)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": final_answer_prompt2.format(problem=problem, steps_str=steps_str),\n",
    "        },\n",
    "    ]\n",
    "    return await agenerate_from_gpt(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the loop to incorporate stopping check\n",
    "# Runs in a while True loop. \n",
    "# Once we have the MIN num, then we check each time for do_we_have_answer check.\n",
    "# Will always generate more unverified steps if there are none left (min 1 step)\n",
    "from utils.async_logger import AsyncLogger\n",
    "\n",
    "\n",
    "async def step_by_step_loop_2(question: str, log_path:str):\n",
    "    AsyncLogger.add_message(log_path, f\"Starting run {log_path}\",)\n",
    "    # More of a suggestion.. used as a starting point. The regeneration will always generate at least 1 step\n",
    "    MIN_STEPS = 5\n",
    "\n",
    "    unverified_steps = []\n",
    "    verified_steps = []\n",
    "    while True:\n",
    "        # Generate if unverified_steps empty.\n",
    "        if not unverified_steps:\n",
    "            desired_steps = max(1, MIN_STEPS - len(verified_steps))\n",
    "            unverified_steps = await create_steps_2(question, verified_steps, desired_steps)\n",
    "            AsyncLogger.add_message(log_path, f\"Generated steps: \\n{unverified_steps}\")\n",
    "\n",
    "        # Verify the current step\n",
    "        current_step = unverified_steps.pop(0)\n",
    "        verify = await verify_step(question, verified_steps, current_step)\n",
    "        AsyncLogger.add_message(log_path, f\"Verifying current step: \\n{current_step}\")\n",
    "        \n",
    "        # If correct, add to verified_steps\n",
    "        if verify.correct:\n",
    "            AsyncLogger.add_message(log_path, \"Step was correct. Add to verified\")\n",
    "            verified_steps.append(current_step)\n",
    "        else:\n",
    "            # Since the step was incorrect, throw away the rest of the unverified steps (triggers regeneration next time)\n",
    "            unverified_steps = []\n",
    "\n",
    "            # Try fixing step -> overwrite current_step\n",
    "            current_step = await fix_step(question, verified_steps, current_step, verify.reasoning)\n",
    "            AsyncLogger.add_message(log_path, f\"Fix the current step for reasoning: {verify.reasoning}\")\n",
    "            AsyncLogger.add_message(log_path, f\"Fixed step: {current_step}\")\n",
    "            \n",
    "            # Verify fixed step\n",
    "            verify_fixed = await verify_step(question, verified_steps, current_step)\n",
    "            AsyncLogger.add_message(log_path, f\"Fixed step was verified as: {verify_fixed.correct}\")\n",
    "            \n",
    "            # Add if correct\n",
    "            if verify_fixed.correct:\n",
    "                verified_steps.append(current_step)\n",
    "            else:\n",
    "                # If the fixed_step is incorrect, skip checking for the answer\n",
    "                continue\n",
    "            \n",
    "        # We want to at least generate 5 steps.. to avoid premature stopping\n",
    "        if len(verified_steps) >= MIN_STEPS:\n",
    "            # Check if all the steps give the answer\n",
    "            if await do_we_have_answer2(question, verified_steps):\n",
    "                AsyncLogger.add_message(log_path, f\"We have answer triggered\")\n",
    "                break\n",
    "                \n",
    "                \n",
    "    # Get the final answer\n",
    "    final_answer = await get_answer_from_steps2(question, verified_steps)            \n",
    "    \n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n{step}\" for i, step in enumerate(verified_steps)])\n",
    "    AsyncLogger.add_message(log_path, \"We have our verified steps:==========\\n\")\n",
    "    AsyncLogger.add_message(log_path, steps_str)\n",
    "    AsyncLogger.add_message(log_path, f\"And our Final Answer\\n{final_answer}\")\n",
    "    \n",
    "    await AsyncLogger.flush_one(log_path)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reuse the async wrapper\n",
    "\n",
    "# Using same question as idea 1\n",
    "# Answer to q1 is 147*sqrt(3)\n",
    "question1 = \"Point $P$ is inside equilateral triangle $ABC$ such that the altitudes from $P$ to $\\overline{AB}$, $\\overline{BC}$, and $\\overline{CA}$ have lengths 8, 6, and 7 respectively.  What is the area of triangle $ABC$?\"\n",
    "\n",
    "# Run # 2\n",
    "await run_many_times_async(step_by_step_loop_2, question1, max_concurrent_tasks=10, runs=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had a lot of trouble with checking that the EXACT answer was included in the step. \n",
    "For some reason it loves to trigger on non exact answer.\n",
    "\n",
    "This is why we have the sanity_check() function.\n",
    "\n",
    "This still doesn't help that much...\n",
    "Let's try making it wait until it hits the max step limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is some old stuff re: math relationships and trying to identify the different types of assumptions. We can try some of it out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analyze the step, and extract the following: \n",
    "# proven mathematical relationships (external)\n",
    "# Problem relationships (conditions and 2nd level conditions). These need to be supported by the question.\n",
    "# Intermediate math relationships - furtther extract from each one,\n",
    "    # proven math rels\n",
    "    # problem rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Verify and *correct* all above. \n",
    "# If a math relationship was wrong, we want to correct it by asking gpt\n",
    "# If a problem relationship was wrong, we want to correct it by asking gpt\n",
    "# If an intermediate math relationship was wrong, we want to correct it by fixing the problem rel or the math rel, and then re-calculating w/ code assistant\n",
    "\n",
    "# We should only calculate each relationship once. So if an intermediate math relationship relies on a proven math rel or a problem rel,\n",
    "# and one of those rels are wrong, it has to be re-calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Now we have verified the step. If it changed we need to re-run the step generation here. We will re-run # 1 but ask it to generate 4\n",
    "# steps now, and give it the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. We should do some other stuff here.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
