{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "132\n",
      "Point $P$ is inside equilateral triangle $ABC$ such that the altitudes from $P$ to $\\overline{AB}$, $\\overline{BC}$, and $\\overline{CA}$ have lengths 5, 6, and 7 respectively.  What is the area of triangle $ABC$?\n",
      "./MATH/test/geometry/990.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Gemotry - t\n",
    "folder_path = \"./MATH/test/geometry/\"\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "# Store each in list\n",
    "json_objects = []\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data[\"file_path\"]=file_path # Add file path so we can keep track of them easily\n",
    "        json_objects.append(json_data)\n",
    "\n",
    "print(len(json_objects))\n",
    "filtered_json_objects = [obj for obj in json_objects if obj.get('level') == 'Level 5']\n",
    "print(len(filtered_json_objects))\n",
    "\n",
    "# Get 5 random ones\n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "samples = random.sample(filtered_json_objects, 5)\n",
    "\n",
    "question = samples[0]['problem']\n",
    "print(question)\n",
    "print(samples[0][\"file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 1 - Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I should try asking it to solve to the end to start, and have it return a list of steps. Then I can go back and see how many are correct?\n",
    "We can limit this by only having it create max 5 steps.\n",
    "\n",
    "Then we can go through step by step. For each step->\n",
    "\n",
    "** For now, avoid this recursive breaking up **\n",
    "\n",
    "1. Should we break this further into steps? is it too detailed?\n",
    "    1. If so, repeat this process and recurse\n",
    "2. If not, then we analyze this step, and extract the following things\n",
    "    - Proven mathematical relationships - written as formulas\n",
    "    - Problem relationships - written without math, just words. Some sort of logical condition in the project\n",
    "    - Intermediate math relationships - numerical relationships between parts of the problem. Only valid in this context. Uses a math formula and some specific knowledge. Break up into a proven math rel and a problem rel\n",
    "\n",
    "3. We need to verify the above. \n",
    "    - Proven math relations can be verified by asking\n",
    "    - Problem relationships verified by asking as well\n",
    "    - Intermediate math relationships should have any math relations verified, then any problem relationships verified, then finally they should be executed and verified with code\n",
    "\n",
    "4. Now the step is good to go. We can re-write it to show the verified above things. We can also store the proven relationships and problem relationships seperately. \n",
    "\n",
    "5. Now if at any point in the larger step we had a problem, we are going to have to recalculate the rest of the steps. If it's the same we can continue through the steps, but if its different it may change the next ones.\n",
    "    - I think this is fine. We need some way of error correcting, and propogating that correction\n",
    "\n",
    "6. Now this is all very good for fixing problems with steps, but what if the whole approach is wrong in some way? Eventually we will build up a list of correct steps, but not all of them will be relevant. And there may be a better order? for some. (Though the order should be fixed by regenerating steps each time.. hmm)\n",
    "Not sure what to do *yet* here\n",
    "\n",
    "Original step by step idea->\n",
    "1. Ask what it would like to do - what is the first step, given what we have.\n",
    "2. Then analyze the step it returns - what is the rationale behind it? What outside theorems does it use? What new assumptions does it make?\n",
    "3. Check each assumption individually, and iterate until we get a good first step.\n",
    "4. Then do this step, update the knowns, and then repeat the process until we get the objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Try and solve the original problem, using max steps. Have it format them w/ schema.\n",
    "# Go step by step..\n",
    "from utils.async_gpt import agenerate_from_gpt_with_schema, agenerate_from_gpt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Steps(BaseModel):\n",
    "    steps: list[str]\n",
    "    summary: str\n",
    "\n",
    "create_steps_prompt = \"\"\"\n",
    "Given the question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\n",
    "Return a series of the next steps to solve the problem. \n",
    "Return a maximum of {steps_left} steps, but it's okay to return less, even just 1 step if that's all it takes to get the solution.\n",
    "Be detailed, and break up complex steps into multiple steps. Try and balance the complexity of each step.\n",
    "Additionally, give a brief summary on the overall strategy, or any key points\n",
    "\"\"\"\n",
    "\n",
    "async def create_steps(question: str, prev_steps: list[str], steps_left:int):\n",
    "    \"\"\"\n",
    "    Create steps to solve a question\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"And the previous steps:\\n {numbered_list}\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": create_steps_prompt.format(question=question, prev_steps_str=prev_steps_str, steps_left=steps_left),\n",
    "        },\n",
    "    ]\n",
    "    steps: Steps = await agenerate_from_gpt_with_schema(\n",
    "        messages, Steps\n",
    "    )\n",
    "\n",
    "    return steps.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I change the question manually, it seems to get it wrong. I should check this out by running 20 or so times each. \n",
    "# THis is kind of expected, after reading that paper about how the models are likely learning common problems...\n",
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Let's go through step by step and see if each step is good. \n",
    "# Generate an explanation for why or why not it's correct. Then return true/false. If any step is wrong, we re-calculate the next steps\n",
    "from utils.async_gpt import agenerate_from_gpt_with_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class VerifyStep(BaseModel):\n",
    "    reasoning: str\n",
    "    correct: bool\n",
    "\n",
    "\n",
    "verify_step_prompt = \"\"\"\n",
    "Given the following info, verify if the CURRENT STEP is correct. \n",
    "Assume the question and any previous steps are correct.\n",
    "ONLY VERIFY the current step.\n",
    "Return the reasoning for why or why not it is correct, and a bool for if it is correct or not\n",
    "\n",
    "Existing Info ====\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\n",
    "New ====\n",
    "Current step:\n",
    "{current_step}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "async def verify_step(question: str, prev_steps: list[str], current_step: str):\n",
    "    \"\"\"\n",
    "    Verify a step given the previous ones. \n",
    "    * Note: we do not care if this step is helpful towards the objective. We're just checking the assumptions it makes.\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        # Turn prev steps to str\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"Previous Steps:\\n {numbered_list}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": verify_step_prompt.format(question=question, prev_steps_str=prev_steps_str, current_step=current_step),\n",
    "        },\n",
    "    ]\n",
    "    verify: VerifyStep = await agenerate_from_gpt_with_schema(\n",
    "        messages, VerifyStep\n",
    "    )\n",
    "\n",
    "    return verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 If a step is wrong, we'll want to correct it.\n",
    "# For now lets try giving the function all the info. We will give it the existing info, the previous incorrect step, and the reasoning behind it\n",
    "from utils.async_gpt import agenerate_from_gpt\n",
    "\n",
    "\n",
    "previous_info_prompt = \"\"\"\n",
    "Existing True Information ====\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "{prev_steps_str}\n",
    "\"\"\"\n",
    "\n",
    "wrong_step_prompt = \"\"\"\n",
    "New Information ===\n",
    "Current step (incorrect):\n",
    "{current_step}\n",
    "\n",
    "Provided reason why current step is incorrect:\n",
    "{reasoning}\n",
    "\"\"\"\n",
    "\n",
    "ask=\"\"\"\n",
    "The existing true information has been verified. It shows the preceding steps.\n",
    "The new information shows the generated current step. This current step contains an error in it. The reason for the error is given.\n",
    "\n",
    "Your job is to rewrite this current step so that it is CORRECT. Use the reason given to fix the current step.\n",
    "Refer back to the existing true information for verified assumptions. Do not add ANY new ideas, or more steps. \n",
    "ONLY change the current step.\n",
    "Fix the current step for the reason provided, and return ONLY the new current step.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def fix_step(question: str, prev_steps: list[str], current_step: str, reasoning: str):\n",
    "    \"\"\"\n",
    "    Fix a step that was said to be wrong. Returns the new step.\n",
    "    Currently we are giving all the information. In the future consider limiting scope\n",
    "    \"\"\"\n",
    "    if len(prev_steps) == 0:\n",
    "        prev_steps_str = \"\"\n",
    "    else:\n",
    "        # Turn prev steps to str\n",
    "        numbered_list = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(prev_steps)])\n",
    "        prev_steps_str = f\"Previous Steps:\\n {numbered_list}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"assistant\",\n",
    "            \"content\": previous_info_prompt.format(question=question, prev_steps_str=prev_steps_str)},\n",
    "\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": wrong_step_prompt.format(current_step=current_step, reasoning=reasoning),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": ask,\n",
    "        },\n",
    "    ]\n",
    "    new_step = await agenerate_from_gpt(\n",
    "        messages\n",
    "    )\n",
    "\n",
    "    return new_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get final answer rom steps\n",
    "\n",
    "async def get_answer_from_steps(steps: list[str]):\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(steps)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\" Given the following solution steps to a problem return ONLY the final answer: \n",
    "            {steps_str}\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    return await agenerate_from_gpt(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.async_logger import AsyncLogger\n",
    "\n",
    "async def step_by_step_loop(question: str, log_path:str):\n",
    "    AsyncLogger.add_message(log_path, f\"Starting run {log_path}\",)\n",
    "    # Max steps is used to try and keep gpt from generating a million steps or 1 step each time.\n",
    "    MAX_STEPS = 5\n",
    "    verified_steps = []\n",
    "    # Create the first steps\n",
    "    unverified_steps = await create_steps(question, [], MAX_STEPS)\n",
    "\n",
    "    while len(unverified_steps) > 0 and len(verified_steps) < MAX_STEPS:\n",
    "        current_step = unverified_steps.pop(0)\n",
    "        AsyncLogger.add_message(log_path, f\"Verify the current step: \\n{current_step}\")\n",
    "        verify = await verify_step(question, verified_steps, current_step)\n",
    "\n",
    "        if not verify.correct:\n",
    "            AsyncLogger.add_message(log_path, f\"Fix the current step for reasoning: {verify.reasoning}\")\n",
    "            fixed_step = await fix_step(question, verified_steps, current_step, verify.reasoning)\n",
    "            AsyncLogger.add_message(log_path, f\"#Fixed step: {fixed_step}\")\n",
    "\n",
    "            # After fixing a step we need to verify it since it often gets off the rails. \n",
    "            # If it's correct we can add it. Else we throw the current step out and regenerate\n",
    "            verify_fixed = await verify_step(question, verified_steps, fixed_step)\n",
    "            AsyncLogger.add_message(log_path, f\"Fixed step was verified as: {verify_fixed.correct}\")\n",
    "    \n",
    "            if verify_fixed.correct:\n",
    "                verified_steps.append(fixed_step)\n",
    "                if len(verified_steps) == MAX_STEPS: # If we have reached the end, break here. #TODO: would be nice to avoid checking len(verified_steps) in two places..\n",
    "                    break\n",
    "            \n",
    "            unverified_steps = await create_steps(question, verified_steps, MAX_STEPS - len(verified_steps))\n",
    "            AsyncLogger.add_message(log_path, f\"Regenerated steps: \\n{unverified_steps}\")\n",
    "\n",
    "        else:\n",
    "            AsyncLogger.add_message(log_path, \"Step was correct. Add to verified\")\n",
    "            verified_steps.append(current_step)\n",
    "\n",
    "\n",
    "    AsyncLogger.add_message(log_path, \"We have our verified steps:==========\\n\")\n",
    "\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n{step}\" for i, step in enumerate(verified_steps)])\n",
    "    AsyncLogger.add_message(log_path, steps_str)\n",
    "\n",
    "    final_answer = await get_answer_from_steps(verified_steps)\n",
    "    AsyncLogger.add_message(log_path, f\"And our Final Answer\\n{final_answer}\")\n",
    "\n",
    "    await AsyncLogger.flush_one(log_path)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async wrapper to run multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "from utils.custom_logger import CustomLogger\n",
    "from typing import Callable, Awaitable\n",
    "\n",
    "\n",
    "async def main(function: Callable[[str, str], Awaitable[str]],question: str, max_concurrent_tasks=10, runs=10):\n",
    "    generated_answers = []\n",
    "    CustomLogger.start_watch() \n",
    "\n",
    "    # Used in case we need to limit for e.g. rate limits\n",
    "    semaphore = anyio.Semaphore(max_concurrent_tasks)\n",
    "\n",
    "    # Async wrapper - edits generated_answers\n",
    "    async def wrapper(question: str, i: int):\n",
    "        log_path = f\"run-{i}\"\n",
    "        async with semaphore: # Limit concurrent tasks\n",
    "            try:\n",
    "                result = await function(question, log_path)\n",
    "                generated_answers.append(f\"{i}: {result}\")\n",
    "                CustomLogger.print(f\"Finished task {i}\") \n",
    "            except Exception as e:\n",
    "                CustomLogger.print(f\"Error on problem {log_path}: {e}\")\n",
    "                AsyncLogger.add_message(log_path, \"Error\", str(e))\n",
    "                AsyncLogger.flush_one(log_path)\n",
    "\n",
    "    async with anyio.create_task_group() as tg:\n",
    "       for i in range(runs):\n",
    "            tg.start_soon(wrapper, question, i) \n",
    "    \n",
    "\n",
    "    CustomLogger.print(\"All answers collected.\")\n",
    "\n",
    "    # Add to validation\n",
    "    CustomLogger.update_path(\"validation\")\n",
    "    CustomLogger.default_log(\"Generated\", *generated_answers)\n",
    "    CustomLogger.default_log(\"Actual\", \"147 * sqrt(3)\")\n",
    "\n",
    "\n",
    "# Test out custom questions\n",
    "# Answer to q1 is 147*sqrt(3)\n",
    "question1 = \"Point $P$ is inside equilateral triangle $ABC$ such that the altitudes from $P$ to $\\overline{AB}$, $\\overline{BC}$, and $\\overline{CA}$ have lengths 8, 6, and 7 respectively.  What is the area of triangle $ABC$?\"\n",
    "\n",
    "\n",
    "await main(step_by_step_loop, question1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 2: Take the step by step and improve on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the first time, we stored some runs in attempt1. \n",
    "\n",
    "3/10 were wrong:\n",
    "- 2 due to incorrect math validation,\n",
    "- 1 due to stopping before we reached the final answer. \n",
    "\n",
    "Fixes:\n",
    "- To fix the math we can add a second validator that looks for mathematical inconsistencies using the code editor.\n",
    "- To fix the premature stopping we can add a check at the end that makes sure we have the exact answer. If not, we will add a step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New validator to check w/ code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have answer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class HaveAnswer(BaseModel):\n",
    "    yes: bool\n",
    "\n",
    "answer_check_prompt = \"\"\" \n",
    "Return yes if and only if we have the exact answer to the following Question in the Solution Steps\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Solution Steps:\n",
    "{steps_str}\n",
    "\"\"\"\n",
    "\n",
    "async def do_we_have_answer(steps: list[str], question):\n",
    "    # Function to get final answer rom steps\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n {step}\" for i, step in enumerate(steps)])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": answer_check_prompt.format(steps_str=steps_str, question=question),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    answer: HaveAnswer = await agenerate_from_gpt_with_schema(messages, HaveAnswer)\n",
    "    return answer.yes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the loop to incorporate premature stopping check\n",
    "from utils.async_logger import AsyncLogger\n",
    "\n",
    "\n",
    "async def step_by_step_loop_2(question, i):\n",
    "    log_path = f\"run-{i}\"\n",
    "    AsyncLogger.add_message(log_path, f\"Starting run #{i}\",)\n",
    "    # Max steps is used to try and keep gpt from generating a million steps or 1 step each time.\n",
    "    MAX_STEPS = 5\n",
    "    verified_steps = []\n",
    "    # Create the first steps\n",
    "    unverified_steps = await create_steps(question, [], MAX_STEPS)\n",
    "\n",
    "    while len(unverified_steps) > 0 and len(verified_steps) < MAX_STEPS:\n",
    "        current_step = unverified_steps.pop(0)\n",
    "        AsyncLogger.add_message(log_path, f\"Verify the current step: \\n{current_step}\")\n",
    "        verify = await verify_step(question, verified_steps, current_step)\n",
    "\n",
    "        if not verify.correct:\n",
    "            AsyncLogger.add_message(log_path, f\"Fix the current step for reasoning: {verify.reasoning}\")\n",
    "            fixed_step = await fix_step(question, verified_steps, current_step, verify.reasoning)\n",
    "            AsyncLogger.add_message(log_path, f\"#Fixed step: {fixed_step}\")\n",
    "\n",
    "            # After fixing a step we need to verify it since it often gets off the rails. \n",
    "            # If it's correct we can add it. Else we throw the current step out and regenerate\n",
    "            verify_fixed = await verify_step(question, verified_steps, fixed_step)\n",
    "            AsyncLogger.add_message(log_path, f\"Fixed step was verified as: {verify_fixed.correct}\")\n",
    "    \n",
    "            if verify_fixed.correct:\n",
    "                verified_steps.append(fixed_step)\n",
    "                if len(verified_steps) == MAX_STEPS: # If we have reached the end, break here. #TODO: would be nice to avoid checking len(verified_steps) in two places..\n",
    "                    break\n",
    "            \n",
    "            unverified_steps = await create_steps(question, verified_steps, MAX_STEPS - len(verified_steps))\n",
    "            AsyncLogger.add_message(log_path, f\"Regenerated steps: \\n{unverified_steps}\")\n",
    "\n",
    "        else:\n",
    "            AsyncLogger.add_message(log_path, \"Step was correct. Add to verified\")\n",
    "            verified_steps.append(current_step)\n",
    "\n",
    "\n",
    "    # TODO: merge this into the above loop somehow. It should be able to do this multiple times + verify each.\n",
    "    # TODO: If this is where a lot of them are failing, we can revisit\n",
    "    # Let's check that our verified steps actualy get us the answer\n",
    "    if_answer = await do_we_have_answer(verified_steps, question)\n",
    "    if not if_answer:\n",
    "        # Generate a new step\n",
    "        last_step = await create_steps(question, verified_steps, 1)\n",
    "        # For now, hope that this step is correct lol. \n",
    "        verified_steps.append(last_step)\n",
    "\n",
    "\n",
    "    AsyncLogger.add_message(log_path, \"We have our verified steps:==========\\n\")\n",
    "\n",
    "    steps_str = \"\\n\".join([f\"Step {i+1}:\\n{step}\" for i, step in enumerate(verified_steps)])\n",
    "    AsyncLogger.add_message(log_path, steps_str)\n",
    "\n",
    "    final_answer = await get_answer_from_steps(verified_steps)\n",
    "    AsyncLogger.add_message(log_path, f\"And our Final Answer\\n{final_answer}\")\n",
    "\n",
    "    await AsyncLogger.flush_one(log_path)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is some old stuff re: math relationships and trying to identify the different types of assumptions. We can try some of it out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analyze the step, and extract the following: \n",
    "# proven mathematical relationships (external)\n",
    "# Problem relationships (conditions and 2nd level conditions). These need to be supported by the question.\n",
    "# Intermediate math relationships - furtther extract from each one,\n",
    "    # proven math rels\n",
    "    # problem rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Verify and *correct* all above. \n",
    "# If a math relationship was wrong, we want to correct it by asking gpt\n",
    "# If a problem relationship was wrong, we want to correct it by asking gpt\n",
    "# If an intermediate math relationship was wrong, we want to correct it by fixing the problem rel or the math rel, and then re-calculating w/ code assistant\n",
    "\n",
    "# We should only calculate each relationship once. So if an intermediate math relationship relies on a proven math rel or a problem rel,\n",
    "# and one of those rels are wrong, it has to be re-calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Now we have verified the step. If it changed we need to re-run the step generation here. We will re-run # 1 but ask it to generate 4\n",
    "# steps now, and give it the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. We should do some other stuff here.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
